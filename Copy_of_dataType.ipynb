{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJiwmgmnQswfgWY/XkuQJU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myk93/String-Data-Type-Inference/blob/main/Copy_of_dataType.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JFbX14EUzZvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import math"
      ],
      "metadata": {
        "id": "8---mfRIp-ot"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ],
      "metadata": {
        "id": "MlLkgUimqdsb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "F6Y-Bvuvrxvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfc0c83-b006-4763-d26f-7db42934e107"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/input/words.txt'\n",
        "text =pd.read_csv(path, sep =\" \")\n"
      ],
      "metadata": {
        "id": "T1KU8tuku248"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_directory_path(path_to_create):\n",
        "    os.mkdir(path_to_create)\n",
        "\n",
        "\n",
        "def dict_to_string(dictionary):\n",
        "    string_representation = \"\"\n",
        "    for key, value in dictionary.items():\n",
        "        string_representation += f\"{key}: {value}\\n\"\n",
        "    return string_representation\n",
        "\n",
        "\n",
        "def save_string_to_file(file_name, content):\n",
        "    with open(file_name, 'w') as file:\n",
        "        file.write(content)\n",
        "\n",
        "\n",
        "def save_model(file_path, model_to_save):\n",
        "    # save the iris classification model as a pickle file\n",
        "    model_pkl_file = file_path + \".pkl\"\n",
        "\n",
        "    with open(model_pkl_file, 'wb') as file:\n",
        "        pickle.dump(model_to_save, file)\n",
        "\n",
        "\n",
        "def random_date():\n",
        "    symbols = ['/', ':', '-', ' ', '.']\n",
        "    symbol = symbols[random.randint(0, len(symbols) - 1)]\n",
        "    stiles = [('mm' + symbol + 'dd' + symbol + 'yy'),\n",
        "              ('mm' + symbol + 'dd' + symbol + 'yyyy'),\n",
        "              ('dd' + symbol + 'mm' + symbol + 'yy'),\n",
        "              ('dd' + symbol + 'mm' + symbol + 'yyyy'),\n",
        "              ('dd' + symbol + 'yy' + symbol + 'mm'),\n",
        "              ('dd' + symbol + 'yyyy' + symbol + 'mm'),\n",
        "              ('mm' + symbol + 'yy' + symbol + 'dd'),\n",
        "              ('mm' + symbol + 'yyyy' + symbol + 'dd'),\n",
        "              ('yy' + symbol + 'dd' + symbol + 'mm'),\n",
        "              ('yyyy' + symbol + 'dd' + symbol + 'mm'),\n",
        "              ('yy' + symbol + 'mm' + symbol + 'dd'),\n",
        "              ('yyyy' + symbol + 'mm' + symbol + 'dd')\n",
        "              ]\n",
        "    x = random.randint(0, len(stiles) - 1)\n",
        "    format_of_date = stiles[x]\n",
        "    replaced_string = re.sub(r'mm', str(random.randint(1, 12)), format_of_date)\n",
        "    replaced_string = re.sub(r'dd', str(random.randint(1, 31)), replaced_string)\n",
        "    if x % 2:\n",
        "        replaced_string = re.sub(r'yyyy', str(random.randint(1900, 2100)), replaced_string)\n",
        "    else:\n",
        "        replaced_string = re.sub(r'yy', str(random.randint(0, 99)), replaced_string)\n",
        "    return replaced_string\n",
        "\n",
        "\n",
        "def generate_random_strings(filename_string, k):\n",
        "\n",
        "    random_strings = []\n",
        "    for _ in range(k):\n",
        "        num_words = random.randint(1, 20)  # Generate a random number of words between 1 and 50\n",
        "        selected_words = random.choices(text, k=num_words)  # Select random words from the list\n",
        "        random_string = ' '.join(selected_words)  # Join the selected words into a single string\n",
        "        random_strings.append(random_string)\n",
        "    return random_strings\n",
        "\n",
        "\n",
        "def generate_random_booleans(k):\n",
        "    bools = []\n",
        "    for _ in range(k):\n",
        "        word = (['true', 'false'])[random.randint(0, 1)]\n",
        "        word = ''.join(chr(ord(word[i]) - (ord('a') - ord('A')) * random.randint(0, 1)) for i in range(len(word)))\n",
        "        bools.append(word)\n",
        "    return bools\n",
        "\n"
      ],
      "metadata": {
        "id": "Zj3n3D3E18OV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_directory_path('/content/gdrive/MyDrive/out/')"
      ],
      "metadata": {
        "id": "E3YErrGmjJtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ver = str(7)\n",
        " path = '/content/gdrive/MyDrive/out/ver '+ver+'/'\n",
        " create_directory_path(path)\n",
        "\n"
      ],
      "metadata": {
        "id": "oJA5eJKxt3ID"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    data = {\n",
        "        'strings': [\n",
        "          '1.1','1.8'\n",
        "        ],\n",
        "        'data_types': [\n",
        "          'float','float'\n",
        "        ]\n",
        "    }\n",
        "    size = 15000\n",
        "    filename = r'C:\\Users\\menik\\Downloads\\words.txt'\n",
        "    #strings = generate_random_strings(filename, size)\n",
        "    booleans = generate_random_booleans(size)\n"
      ],
      "metadata": {
        "id": "uhz0W-xFt4ND"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # adding size*6 integers\n",
        "    for _ in range(size*4):  # Change the range value to the desired number of integers\n",
        "        data['strings'].append(\n",
        "            str(random.randint(1, 1000000)))  # Add the random integer to the array\n",
        "        data['data_types'].append('integer')\n",
        "\n"
      ],
      "metadata": {
        "id": "YXzcDfFC2WYc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # adding size*6 floats\n",
        "    for _ in range(math.floor(size*2)):  # Change the range value to the desired number of float\n",
        "        data['strings'].append(\n",
        "            str(float(round(random.uniform(-1, 1) * random.randint(100000, 10000000),\n",
        "                            random.randint(1, 10)))))  # Add the random float to the array\n",
        "        data['data_types'].append('float')\n",
        "        data['strings'].append(\n",
        "            str(float(round(random.uniform(-1, 1) * random.randint(1000, 100000),\n",
        "                            random.randint(1, 10)))))  # Add the random float to the array\n",
        "        data['data_types'].append('float')\n",
        "        data['strings'].append(\n",
        "            str(float(round(random.uniform(-1, 1) * random.randint(10, 1000),\n",
        "                            random.randint(0, 5)))))  # Add the random float to the array\n",
        "        data['data_types'].append('float')\n",
        "        data['strings'].append(\n",
        "            str(float(round(random.uniform(-1, 1) * random.randint(1, 9),\n",
        "                            random.randint(0, 5)))))  # Add the random float to the array\n",
        "        data['data_types'].append('float')\n"
      ],
      "metadata": {
        "id": "eq4bkYT3bCy6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # adding size/2 dates\n",
        "    for _ in range(math.floor(size / 2)):  # Change the range value to the desired number of date\n",
        "        data['strings'].append(random_date())  # Add the random date to the array\n",
        "        data['data_types'].append('date')"
      ],
      "metadata": {
        "id": "628ovUXqbC7b"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # adding 2*size string\n",
        "    for _ in range(size * 2):  # Change the range value to the desired number of string\n",
        "        data['strings'].append(strings.pop())  # Add the random string to the array\n",
        "        data['data_types'].append('string')"
      ],
      "metadata": {
        "id": "v6Q0QFmfbDAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # adding 100 bool\n",
        "    for _ in range(math.floor(100)):  # Change the range value to the desired number of integers\n",
        "        data['strings'].append(booleans.pop())  # Add the random booleans to the array\n",
        "        data['data_types'].append('boolean')"
      ],
      "metadata": {
        "id": "i7creT6xbDED"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXkEHvEVbDHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q12-sC7VbDKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Create feature vectors from the strings\n",
        "    # Prepare the data\n",
        "    X = data['strings']\n",
        "    y = data['data_types']\n",
        "    # Create feature vectors from the strings\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_vectorized = vectorizer.fit_transform(X)\n",
        "    num_of_types = np.unique(np.array(data['data_types'])).size\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.10)\n"
      ],
      "metadata": {
        "id": "Tg870Rlc2Xll"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # feature standardization\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    scaler.fit(X_train)\n",
        "\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "ORReWlOQuNVL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    pipeline = [\n",
        "        #[KNeighborsClassifier(n_neighbors=num_of_types), 'KNeighborsClassifier'],\n",
        "        [LogisticRegression(C=1.0,\n",
        "                            solver='lbfgs',\n",
        "                            multi_class='multinomial',\n",
        "                            max_iter=10000), 'LogisticRegression'],\n",
        "        #[DecisionTreeClassifier(), 'DecisionTreeClassifier'],\n",
        "        #[RandomForestClassifier(), 'RandomForestClassifier'],\n",
        "        [SVC(max_iter=1000000), 'svc']\n",
        "    ]"
      ],
      "metadata": {
        "id": "BuaovC0D8hgN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    for model, name in pipeline:\n",
        "        print(\"starting model \" + name)\n",
        "        model.fit(X_train, y_train)\n",
        "        # make predictions on the testing data\n",
        "        y_predict = model.predict(X_test)\n",
        "        create_directory_path(path + name)\n",
        "\n",
        "        save_model(path  + name+ '/model' , model)\n",
        "        save_model(path + name+ '/vectorizer', vectorizer)\n",
        "        # check results\n",
        "        s = name + \" summary\"\n",
        "        s += \"\\nscalar transform \"\n",
        "        s += \"\\n\\n\\n\" + str(confusion_matrix(y_test, y_predict))\n",
        "        s += \"\\n\\n\\n\" + str(classification_report(y_test, y_predict))\n",
        "        s += \"\\n\\n\\n\"\n",
        "        new_strings = [ '105560', '2.718', 'False', '12345', '1993-10-28','11.432','1111.1111','1.6','1.7','12/06/2017']\n",
        "        new_strings_vectorized = vectorizer.transform(new_strings)\n",
        "        predicted_data_types = model.predict(new_strings_vectorized)\n",
        "        # Print the predictions\n",
        "\n",
        "        for string, data_type in zip(new_strings, predicted_data_types):\n",
        "            s += \"\\n\" + f'String: {string} - Predicted data type: {data_type}'\n",
        "        s += \"\\n\" + dict_to_string(data)\n",
        "        print(s)\n",
        "        with open(path + name+ '/summary.txt', 'w') as f:\n",
        "            f.write(s)\n",
        "        #save_string_to_file(path + 'Summery.txt', s)\n",
        "        print(\"finished model \" + name)"
      ],
      "metadata": {
        "id": "XBdR5CIW2bZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0258acfd-7951-4bd1-8881-9ab5f80bdbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting model LogisticRegression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    #with open('/content/gdrive/MyDrive/out/ver 1/LogisticRegression/model.pkl') as file:\n",
        "    file = open('/content/gdrive/MyDrive/out/ver 5/svc/model.pkl', 'rb')\n",
        "    model = pickle.load(file)\n",
        "    with open('/content/gdrive/MyDrive/out/ver 5/svc/vectorizer.pkl', 'rb') as file:\n",
        "        vectorizer = pickle.load(file)\n",
        "    # evaluate model\n",
        "    array_to_eval =['1.87','1.15','12:2:3']\n",
        "    y_predict = model.predict(vectorizer.transform(array_to_eval))\n",
        "    for string, data_type in zip(array_to_eval, y_predict):\n",
        "      print(f'String: {string} - Predicted data type: {data_type}')\n"
      ],
      "metadata": {
        "id": "eUTMTL7T2h_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a145650-3e5c-445d-b9b8-53fc7690aa26"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String: 1.87 - Predicted data type: float\n",
            "String: 1.15 - Predicted data type: float\n",
            "String: 12:2:3 - Predicted data type: float\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the predictions\n",
        "  for string, data_type in zip(new_strings, predicted_data_types):\n",
        "    print(f'String: {string} - Predicted data type: {data_type}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "lc7UCgfh2lQ0",
        "outputId": "3def088f-c6b7-42e3-fd80-155cd3681a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-04f318eaac00>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for string, data_type in zip(new_strings, predicted_data_types):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}